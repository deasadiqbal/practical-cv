{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the available classes: ['E:\\\\\\\\computer vision\\\\\\\\practical-cv\\\\\\\\dataset\\\\\\\\train\\\\daisy', 'E:\\\\\\\\computer vision\\\\\\\\practical-cv\\\\\\\\dataset\\\\\\\\train\\\\dandelion', 'E:\\\\\\\\computer vision\\\\\\\\practical-cv\\\\\\\\dataset\\\\\\\\train\\\\rose', 'E:\\\\\\\\computer vision\\\\\\\\practical-cv\\\\\\\\dataset\\\\\\\\train\\\\sunflower', 'E:\\\\\\\\computer vision\\\\\\\\practical-cv\\\\\\\\dataset\\\\\\\\train\\\\tulip']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "def read_and_decode(filename, reshape_dims):\n",
    "  # Read the file\n",
    "  img = tf.io.read_file(filename)\n",
    "  # Convert the compressed string to a 3D uint8 tensor.\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # Resize the image to the desired size.\n",
    "  return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "CLASS_NAMES = [item.numpy().decode(\"utf-8\") for item in \n",
    "               tf.strings.regex_replace(\n",
    "                 tf.io.gfile.glob(r\"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train/*\"),\n",
    "                 r'E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train/', \"\")]\n",
    "CLASS_NAMES = [item for item in CLASS_NAMES if item.find(\".\") == -1]\n",
    "print(\"These are the available classes:\", CLASS_NAMES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'daisy', shape=(), dtype=string) tf.Tensor([0.6474569  0.6199989  0.50390774], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def decode_csv(csv_row):\n",
    "  record_defaults = [\"path\", \"flower\"]\n",
    "  filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
    "  img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
    "  # label = tf.math.equal(CLASS_NAMES, label_string)\n",
    "  return img, label_string\n",
    "\n",
    "dataset = (tf.data.TextLineDataset(\n",
    "    \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train_csv.csv\").map(decode_csv))\n",
    "\n",
    "for img, label in dataset.take(1):\n",
    "  avg = tf.math.reduce_mean(img, axis=[0, 1]) # average pixel in the image\n",
    "  print(label, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_csv(csv_row):\n",
    "  record_defaults = [\"path\", \"flower\"]\n",
    "  filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
    "  img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
    "  label = tf.argmax(tf.math.equal(CLASS_NAMES, label_string))\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.TextLineDataset(\n",
    "    \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train_csv.csv\").\n",
    "    map(decode_csv)).batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (tf.data.TextLineDataset(\n",
    "    \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\val.csv\").\n",
    "    map(decode_csv)).batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in this function i basiclly defining our nural network with learning rate and regulraization \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(batch_size = 3,\n",
    "                       lrate = 0.001, # default in Adam constructor\n",
    "                       l1 = 0,\n",
    "                       l2 = 0,\n",
    "                       num_hidden = 12):\n",
    "    \n",
    "    regulrizer = tf.keras.regularizers.l1_l2(l1, l2)\n",
    "\n",
    "    train_dataset = (tf.data.TextLineDataset(\n",
    "            \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train_csv.csv\").map(decode_csv)).batch(batch_size),\n",
    "\n",
    "    test_dataset = (tf.data.TextLineDataset(\n",
    "            \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\val.csv\").map(decode_csv)).batch(batch_size),\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape =  (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "        tf.keras.layers.Dense(num_hidden, kernel_regularizer= regulrizer, activation= tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(len(CLASS_NAMES), kernel_regularizer= regulrizer, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lrate),\n",
    "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(train_dataset, validation_data=test_dataset, epochs=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_and_evaluate(batch_size=3, lrate=0.0001, l1=0, l2=0, num_hidden=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter tuning is the process of finding the configuration of hyperparameters that results in the best performance.\n",
    "Hyperparameters are the variables that control the training process and the topology of an ML model.\"\"\"\n",
    "#we use the hyperparameter tuning when we fit the model\n",
    "\n",
    "import tensorflow as tf\n",
    "def build_model(hp):    #hp is the hyperparameter\n",
    "    lrate = hp.Float('lrate', 1e-4, 1e-2, sampling = 'log')\n",
    "    l1 = 0\n",
    "    l2 = hp.Choice('l2', values=[0.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "    num_hidden = hp.Int('num_hidden', min_value = 32, max_value = 512, step = 32)\n",
    "\n",
    "    regulazier = tf.keras.regularizers.L1L2(l1 = l1, l2 = l2)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "        tf.keras.layers.Dense(num_hidden, kernal_regularizer = regulazier, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(len(CLASS_NAMES), kernal_regularizer = regulazier, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    #class name is represent the no. of classes are present in our 5-flower dataset\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizer.Adam(learning_rate = lrate),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.TextLineDataset(\n",
    "        \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\train_csv.csv\").map(decode_csv)).batch(3)\n",
    "\n",
    "test_dataset = (tf.data.TextLineDataset(\n",
    "    \"E:\\\\computer vision\\\\practical-cv\\\\dataset\\\\val.csv\").map(decode_csv)).batch(3)\n",
    "\"\"\"\n",
    "We pass the build_model() function into a Keras Tuner optimization algorithm. Sev‚Äê\n",
    "eral algorithms are supported, but Bayesian optimization is an old standby that works\n",
    "well for computer vision problems:\n",
    "\"\"\"\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective = ['vak_accuracy', 'max'],\n",
    "    max_trials = 10,\n",
    "    num_initial_points = 2\n",
    "    over_write = False ) #if true then it will overwrite the existing logs and checkpoints\n",
    "\n",
    "\"\"\"\n",
    "Here, we are specifying that our objective is to maximize the validation accuracy and\n",
    "that we want the Bayesian optimizer to run 10 trials starting from 2 randomly chosen\n",
    "seed points. The tuner can pick up where it left off, and we are asking Keras to do so\n",
    "by telling it to reuse information learned in preexisting trials and not start with a\n",
    "blank slate.\n",
    "Having created the tuner, we can then run the search:\n",
    "\"\"\"\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data = test_dataset,\n",
    "    epochs = 5\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2)]\n",
    "    #patience is the number of epochs with no improvement after which training will be stopped\n",
    ")\n",
    "'''\n",
    "At the end of the run, we can get the top N trials (the ones that ended with the highest\n",
    "validation accuracy) using:\n",
    "'''\n",
    "topN = 2\n",
    "for x in range(topN):\n",
    "    print(tuner.get_best_parameters(topN)[x].values)\n",
    "    print(tuner.get_best_models(topN)[x].summary)\n",
    "\n",
    "# output will look like this {'lrate': 0.00017013245197465996, 'l2': 0.0, 'num_hidden': 64}\n",
    "\n",
    "\"\"\"When we did hyperparameter tuning for the 5-flowers problem, we determined that\n",
    "the best set of parameters was:\n",
    "{'lrate': 0.00017013245197465996, 'l2': 0.0, 'num_hidden': 64}\n",
    "The best validation accuracy obtained was 0.46.\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
