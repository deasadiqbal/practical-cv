{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Enable GPU and set up helper functions\n","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2023-07-13T17:35:31.403641Z","iopub.execute_input":"2023-07-13T17:35:31.403934Z","iopub.status.idle":"2023-07-13T17:35:46.096021Z","shell.execute_reply.started":"2023-07-13T17:35:31.403908Z","shell.execute_reply":"2023-07-13T17:35:46.094839Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.version.VERSION)\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU not found')\nprint('GPU found {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T17:35:46.098548Z","iopub.execute_input":"2023-07-13T17:35:46.099282Z","iopub.status.idle":"2023-07-13T17:35:56.680428Z","shell.execute_reply.started":"2023-07-13T17:35:46.099246Z","shell.execute_reply":"2023-07-13T17:35:56.679458Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"2.12.0\nGPU found /device:GPU:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read Oxford-IIIT Pets dataset\n\nThe dataset is part of TensorFlow datasets.\nVersion 3 and higher of the dataset has ground truth\nsegmentation masks.\n\nThe dataset already contains test and train splits.","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T17:35:56.681761Z","iopub.execute_input":"2023-07-13T17:35:56.682553Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 773.52 MiB (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b30b98b08c4d6bac678854e877ea74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3ff4252f7df41a982ced49e3f95743e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f380cdd5097f4c348565f39bc25aa8e3"}},"metadata":{}}]},{"cell_type":"code","source":"def read_and_preprocess(data):\n    input_img = tf.image.resize(data['image'], (128,128)) #resizing the image shape\n    input_mask = tf.image.resize(data['segmentation_mask'], (128, 128))  #resizing the mask shape\n    \n    input_img = tf.image.convert_image_dtype(input_img, tf.float32) #converting image to 0,1 form\n    input_mask -=1 #from {1,2,3} to {0,1,2}\n    return input_img , input_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dataset['train'].map(read_and_preprocess, num_parallel_calls = tf.data.AUTOTUNE)\ntest = dataset['test'].map(read_and_preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display some example images and their labels","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nf, ax = plt.subplots(2, 5, figsize = (16,5))\nfor idx,(img, mask) in enumerate(train.take(5)):\n    ax[0, idx].imshow(tf.keras.preprocessing.image.array_to_img(img))\n    ax[0, idx].axis('off')\n    mask = tf.reshape(mask, [128,128])\n    ax[1, idx].imshow(mask.numpy())\n    ax[1, idx].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modified U-Net\nThe model being used here is a modified [U-Net](https://link.springer.com/content/pdf/10.1007%2F978-3-319-24574-4_28.pdf).\nA U-Net typically consists of an encoder \nwhich downsamples an image to an encoding,\nand a mirrored decoder which upsamples the encoding\nback to the desired mask. The decoder blocks have a number of skip connections\nthat directly connect the encoder blocks to the decoder.\n\nDoing this from scratch requires a lot of trainable parameters \n\nIn this notebook, we will use a pretrained MobileNetV2 to\ncreate the encoding and a set of upsampling layers to\nget back to the desired mask. \nWhen doing so, however, we will pull out layers with the\ndesired sizes so that the upsampling skip layers use\ncorresponding weights from the pretrained models.\n\nTo make the dataset go a little bit farther,\nwe also augment the training dataset to flip the images.\n\nNote that the output consists of 3 channels. This is because we have 3 possible labels for each pixel (background, outline, interior). Think of this as multi-classification problem with three possible classes.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\ndef agument(img, mask):\n    if tf.random.uniform(()) > 0.5:\n        img = tf.image.flip_left_right(img)\n        mask = tf.image.flip_left_right(mask)\n    return img, mask\n\ntrain_dataset = train.cache().map(agument).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n\ntest_dataset = test.batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_LENGHT = info.splits['train'].num_examples\nSTEP_PER_EPOCHS = TRAIN_LENGHT // BATCH_SIZE\nOUTPUT_CHANNLE = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### base model\n - we will use mobilenetv2 as a base model","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(input_shape = [128,128, 3], include_top = False)\n\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_output = [base_model.get_layer(name).output for name in layer_names]\n\n# create feature extraction model\ndown_stack = tf.keras.Model(inputs = base_model.input, outputs = base_model_output,\n                           name = 'pretrained_mobilenet')\ndown_stack.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The decoder/upsampler is a series of upsample blocks.\nConv2DTranspose is what does the upsampling.\nWe also add batchnorm and relu.","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow.keras import Sequential, layers\n\ndef upsample(filters, size, name):\n    return Sequential([\n        layers.Conv2DTranspose(filters, size, strides = 2, padding = 'same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n    ], name = name)\nup_stack = [\n    upsample(512, 3, 'upsample_4x4_to_8x8'),\n    upsample(256, 3, 'upsample_8x8_to_16x16'),\n    upsample(128, 3, 'upsample_16x16_to_32x32'),\n    upsample(64, 3,  'upsample_32x32_to_64x64')\n]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#u_t\nimport re\ndef u_net(output_channles):\n    inputs = layers.Input(shape = [128, 128, 3], name = 'input_image')\n    \n    #dwon_sampling\n    skips = down_stack(inputs)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n    \n    #upsampling\n    for idx, (up, skip) in enumerate(zip(up_stack, skips)):\n        x = up(x)\n        concat = layers.Concatenate(name= 'expand_{}'.format(idx))\n        x = concat([x, skip])\n        \n    #last layer of model\n    last = layers.Conv2DTranspose(output_channles, 3, strides = 2, padding = 'same')\n    x = last(x)\n    return tf.keras.Model(inputs = inputs, outputs = x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = u_net(OUTPUT_CHANNLE)\ntf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = u_net(OUTPUT_CHANNLE)\nmodel.compile( optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits =True), metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mask(pred_mask):    \n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display helper functions\ndef display(display_list):\n    plt.figure(figsize=(15, 15))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n        plt.show()\n\ndef show_predictions(dataset, num):\n    for image, mask in dataset.take(num):\n        pred_mask = model.predict(image)\n        display([image[0], mask[0], create_mask(pred_mask)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(train.batch(1), 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To observe how the model improves while it is training we define a callback function to show the predictions when the epoch finishes.","metadata":{}},{"cell_type":"code","source":"class DisplayCallback(tf.keras.callbacks.Callback):\n      def on_epoch_end(self, epoch, logs=None):         \n            if epoch%5 == 0:\n              # clear_output(wait=True) # if you want replace the images each time, uncomment this\n              show_predictions(train.batch(1), 1)\n            print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 15\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\nmodel_history = model.fit(train_dataset,\n                         steps_per_epochs = STEP_PER_EPOCHS,\n                         validation_steps = VALIDATION_STEPS,\n                         validation_data = test_dataset,\n                         callbacks = [DisplayCallback()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = model_history.history['loss']\nval_loss = model_history['val_loss']\nepochs = range(EPOCHS)\n\nplt.figure()\nplt.plot(epochs, loss,'r', label = 'training loss')\nplt.plot(epochs, val_loss, 'bo', label = 'validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}