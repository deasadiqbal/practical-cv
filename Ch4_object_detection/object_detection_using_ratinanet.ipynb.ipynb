{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h0JTdx7zssVc"
      },
      "source": [
        "# Object Detection\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dIT9DF_vsz2n"
      },
      "source": [
        "### import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rNcsB8K-8AG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_jHFpTUTtdrf"
      },
      "source": [
        "### Downloading the COCO2017 dataset\n",
        "-\n",
        "Training on the entire COCO2017 dataset which has around 118k images takes a lot of time, hence we will be using a smaller subset of ~500 images for training in this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3n7sxVYssGE"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
        "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
        "keras.utils.get_file(filename, url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt9Ul7g2uC77"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"data.zip\" , \"r\") as f:\n",
        "  f.extractall(\"./\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zuOFzj8Ywn8Y"
      },
      "source": [
        "### Implementing utility functions\n",
        "Bounding boxes can be represented in multiple ways, the most common formats are:\n",
        "\n",
        "- Storing the coordinates of the corners [xmin, ymin, xmax, ymax]\n",
        "- Storing the coordinates of the center and the box dimensions [x, y, width, height]\n",
        "\n",
        "Since we require both formats, we will be implementing functions for converting between the formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KWzBPxn_uZX7"
      },
      "outputs": [],
      "source": [
        "def swap_xy(boxes):\n",
        "  return tf.stack([boxes[:,1], boxes[:,0], boxes[:,3], boxes[:,2]], axis = -1)\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "  return tf.concat(\n",
        "      [(boxes[...,2:] + boxes[...,:2]) / 2.0, boxes[...,2:] - boxes[...,:2]], axis = -1\n",
        "  )\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "  return tf.concat(\n",
        "      [boxes[...,2:] - boxes[...,:2] / 2.0, boxes[...,2:] + boxes[...,:2] / 2.0], axis = -1\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eQQI8NKB0kwU"
      },
      "source": [
        "### IOU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SZSxbVVszs4V"
      },
      "outputs": [],
      "source": [
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_a3lEZjn2VAv"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-jv7sHquhR"
      },
      "source": [
        "### Implementing Anchor generator\n",
        "-\n",
        "Anchor boxes are fixed sized boxes that the model uses to predict the bounding box for an object. It does this by regressing the offset between the location of the object's center and the center of an anchor box, and then uses the width and height of the anchor box to predict a relative scale of the object. In the case of RetinaNet, each location on a given feature map has nine anchor boxes (at three scales and three ratios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pMiQlXfm3a1V"
      },
      "outputs": [],
      "source": [
        "class AnchorBox():\n",
        "  def __init__(self):\n",
        "    self.aspect_ratios  = [0.5, 1.0, 2.0]\n",
        "    self.sclaes = [2 ** x for x in [0, 1/3, 2/3]]\n",
        "    self.num_anchor = len(self.aspect_ratios) * len(self.sclaes)\n",
        "    self.strides = [2 ** i for i in range(3, 8)]\n",
        "    self.areas = [ x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "    self._anchor_dims =  self._compute_dims()\n",
        "\n",
        "  def _compute_dims(self):\n",
        "    anchor_dims_all = []\n",
        "    for area in self.areas:\n",
        "      anchor_dims = []\n",
        "      for ratio in self.aspect_ratios:\n",
        "        anchor_height = tf.math.sqrt(area/ratio)\n",
        "        anchor_width = area / anchor_height\n",
        "        dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "        for scale in self.sclaes:\n",
        "          anchor_dims.append(scale*dims)\n",
        "      anchor_dims_all.append(tf.stack(anchor_dims, axis =-2))\n",
        "    return anchor_dims_all\n",
        "\n",
        "  def _get_anchors(self, feature_height, feature_width, level):\n",
        "    rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "    ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "    centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "    centers = tf.expand_dims(centers, axis=-2)\n",
        "    centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "    dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "    anchors = tf.concat([centers, dims], axis=-1)\n",
        "    return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "      )\n",
        "\n",
        "  def get_anchors(self, image_height, image_width):\n",
        "    anchors = [\n",
        "        self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "    for i in range(3, 8)\n",
        "        ]\n",
        "    return tf.concat(anchors, axis=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ywdNC83Oqml9"
      },
      "source": [
        "### Preprocessing data\n",
        "Preprocessing the images involves two steps:\n",
        "\n",
        "- Resizing the image: Images are resized such that the shortest size is equal to 800 px, after resizing if the longest side of the image exceeds 1333 px, the image is resized such that the longest size is now capped at 1333 px.\n",
        "- Applying augmentation: Random scale jittering and random horizontal flipping\n",
        "are the only augmentations applied to the images.\n",
        "\n",
        "Along with the images, bounding boxes are rescaled and flipped if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QyVUs15snBWL"
      },
      "outputs": [],
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
        "  return image, boxes\n",
        "\n",
        "def resize_and_pad_image(\n",
        "      image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "  ):\n",
        "  image_shape = tf.cast(tf.shape(image)[:2], dtype = tf.float32)\n",
        "  if jitter is not None:\n",
        "    min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "  ratio = min_side / tf.reduce_min(image_shape)\n",
        "  if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "    ratio = max_side / tf.reduce_max(image_shape)\n",
        "  image_shape = ratio * image_shape\n",
        "  image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "  padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "  image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "  return image, image_shape, ratio\n",
        "\n",
        "def preprocess_data(sample):\n",
        "  image = sample[\"image\"]\n",
        "  bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "  class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "  image, bbox = random_flip_horizontal(image, bbox)\n",
        "  image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "  bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "  bbox = convert_to_xywh(bbox)\n",
        "  return image, bbox, class_id\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding labels\n",
        "The raw labels, consisting of bounding boxes and class ids need to be transformed into targets for training. This transformation consists of the following steps:\n",
        "\n",
        "- Generating anchor boxes for the given image dimensions\n",
        "- Assigning ground truth boxes to the anchor boxes\n",
        "- The anchor boxes that are not assigned any objects, are either assigned the background class or ignored depending on the IOU\n",
        "- Generating the classification and regression targets using anchor boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LableEncoder:\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "            self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "    \n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "    \n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backbone RasNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_backbone():\n",
        "    backbone = tf.keras.applications.ResNet50(include_top=False, input_shape=[None, None, 3])\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return keras.Model(inputs = [backbone.input], outputs = [c3_output, c4_output, c5_output])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Feature Pyramid Network as a custom layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "class FeaturePyramid(keras.layers.Layer):\n",
        "    def __init__(self, backbone=None, **kwargs):\n",
        "        super().__init__(name = \"FeaturePyramid\" **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "    \n",
        "    def call(self, image, training = False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(image, training = training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the classification and box regression heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_head(output_filters, bias_init):\n",
        "    head = keras.Sequential([keras.Input(shape = [None, None, 256])])\n",
        "    kernal_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(keras.layers.Conv2D(256, 3, padding = 'same', kernel_initializer=kernal_init))\n",
        "        head.add(keras.layers.ReLU())\n",
        "    head.add(keras.layers.Conv2D(output_filters, 3, 1, padding = 'same', kernel_initializer=kernal_init, bias_initializer=bias_init))\n",
        "    return head\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building RetinaNet using a subclassed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "class RetinaNet(keras.Model):\n",
        "    def __init__(self, num_classes, backbone = None, **kwargs):\n",
        "        super().__init__(name = \"RetinaNet\", **kwargs)\n",
        "        self.fpn = FeaturePyramid(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training = training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(tf.reshape(self.cls_head(feature), [N, -1, self.num_classes]))\n",
        "        cls_outputs = tf.concat(cls_outputs, axis = 1)\n",
        "        box_outputs = tf.concat(box_outputs, axis = 1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis = -1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing a custom layer to decode predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decodepredictions(keras.layers.Layer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes = 80,\n",
        "            confidence_threshold = 0.05,\n",
        "            nms_iou_threshold = 0.5,\n",
        "            max_detections_per_class = 100,\n",
        "            max_detections = 100,\n",
        "            box_variance = [0.1, 0.1, 0.2, 0.2],\n",
        "            **kwargs\n",
        "    ):\n",
        "        super.__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype = tf.float32)\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis = -1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "    \n",
        "    def call(self, image, predictions):\n",
        "        image_shape = tf.cast(tf.shape(image), dtype = tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Smooth L1 loss and Focal Loss as keras custom losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class RatinaNetBoxLoss(tf.losses.Loss):\n",
        "    def __init__(self, delta):\n",
        "        super().__init__(reduction='none', name = \"RetinaNetBoxLoss\")\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis = -1)\n",
        "    \n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super().__init__(reduction = 'none', name = \"RetinaNetClassificationLoss\")\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y_true, logits= y_pred)\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis = -1)\n",
        "    \n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"combined loss for RetinaNet\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes =80, alpha =0.25, gamma =2.0 , delta = 1.0):\n",
        "        super().__init__(reduction = 'auto', name = 'RatinaNetLoss')\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RatinaNetBoxLoss(delta)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up training parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dir = \"retinanet/\"\n",
        "label_encoder = LableEncoder()\n",
        "num_classes = 80\n",
        "batch_size = 2\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries= learning_rate_boundries, values= learning_rates\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing and compiling model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "  196608/94765736 [..............................] - ETA: 50:54"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rasnet50_backbone \u001b[39m=\u001b[39m get_backbone()\n",
            "Cell \u001b[1;32mIn[53], line 2\u001b[0m, in \u001b[0;36mget_backbone\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_backbone\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     backbone \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mResNet50(include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, input_shape\u001b[39m=\u001b[39;49m[\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39m3\u001b[39;49m])\n\u001b[0;32m      3\u001b[0m     c3_output, c4_output, c5_output \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m         backbone\u001b[39m.\u001b[39mget_layer(layer_name)\u001b[39m.\u001b[39moutput\n\u001b[0;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m layer_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mconv3_block4_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconv4_block6_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconv5_block3_out\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m keras\u001b[39m.\u001b[39mModel(inputs \u001b[39m=\u001b[39m [backbone\u001b[39m.\u001b[39minput], outputs \u001b[39m=\u001b[39m [c3_output, c4_output, c5_output])\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\applications\\resnet.py:521\u001b[0m, in \u001b[0;36mResNet50\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m     x \u001b[39m=\u001b[39m stack1(x, \u001b[39m256\u001b[39m, \u001b[39m6\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    519\u001b[0m     \u001b[39mreturn\u001b[39;00m stack1(x, \u001b[39m512\u001b[39m, \u001b[39m3\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 521\u001b[0m \u001b[39mreturn\u001b[39;00m ResNet(\n\u001b[0;32m    522\u001b[0m     stack_fn,\n\u001b[0;32m    523\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    524\u001b[0m     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    525\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mresnet50\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    526\u001b[0m     include_top,\n\u001b[0;32m    527\u001b[0m     weights,\n\u001b[0;32m    528\u001b[0m     input_tensor,\n\u001b[0;32m    529\u001b[0m     input_shape,\n\u001b[0;32m    530\u001b[0m     pooling,\n\u001b[0;32m    531\u001b[0m     classes,\n\u001b[0;32m    532\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    533\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\applications\\resnet.py:232\u001b[0m, in \u001b[0;36mResNet\u001b[1;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m         file_name \u001b[39m=\u001b[39m (\n\u001b[0;32m    229\u001b[0m             model_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m         file_hash \u001b[39m=\u001b[39m WEIGHTS_HASHES[model_name][\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 232\u001b[0m     weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39;49mget_file(\n\u001b[0;32m    233\u001b[0m         file_name,\n\u001b[0;32m    234\u001b[0m         BASE_WEIGHTS_PATH \u001b[39m+\u001b[39;49m file_name,\n\u001b[0;32m    235\u001b[0m         cache_subdir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    236\u001b[0m         file_hash\u001b[39m=\u001b[39;49mfile_hash,\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    238\u001b[0m     model\u001b[39m.\u001b[39mload_weights(weights_path)\n\u001b[0;32m    239\u001b[0m \u001b[39melif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\data_utils.py:346\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m         urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    347\u001b[0m     \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    348\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\data_utils.py:87\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     85\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     86\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m---> 87\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunk_read(response, reporthook\u001b[39m=\u001b[39mreporthook):\n\u001b[0;32m     88\u001b[0m         fd\u001b[39m.\u001b[39mwrite(chunk)\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\data_utils.py:76\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[1;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[0;32m     74\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     chunk \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread(chunk_size)\n\u001b[0;32m     77\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m reporthook \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mc:\\Users\\deali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "rasnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "model = RetinaNet(num_classes, backbone = rasnet50_backbone) #maybe we get some error here because of the backbone is not checked on data\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor = \"loss\",\n",
        "        save_best_only = False,\n",
        "        save_weights_only = True,\n",
        "        verbose = 1,\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the COCO2017 dataset using TensorFlow Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tfds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (train_dataset, val_dataset), dataset_info \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcoco/2017\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m], with_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
          ]
        }
      ],
      "source": [
        "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
        "    'coco/2017', split=['train', 'validation'], with_info=True, data_dir='data'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up a tf.data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "auto = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=auto)\n",
        "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
        "train_dataset = train_dataset.padded_batch(batch_size = batch_size, padding_values = (0.0, 1e-8, -1), drop_remainder = True)\n",
        "train_dataset = train_dataset.map(label_encoder.encode_batch, num_parallel_calls=auto)\n",
        "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "train_dataset = train_dataset.prefetch(auto)\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=auto)\n",
        "val_dataset = val_dataset.padded_batch(batch_size = 1, padding_values = (0.0, 1e-8, -1), drop_remainder = True)\n",
        "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=auto)\n",
        "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "val_dataset = val_dataset.prefetch(auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "model.fit(\n",
        "    train_dataset.take(100),\n",
        "    validation_data = val_dataset.take(50),\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks_list,\n",
        "    verbose = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_dir = \"data\"\n",
        "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
        "model.load_weights(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building inference model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = tf.keras.Image(shape = [None, None, 3], name = 'image')\n",
        "predictions = model(image, training = False)\n",
        "detection = Decodepredictions(confidence_threshold = 0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs = image, outputs = detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_img(image):\n",
        "    image,_,ratio = resize_and_pad_image(image, jitter = None)\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis = 0), ratio\n",
        "\n",
        "val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
        "int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n",
        "\n",
        "for sample in val_dataset.take(3):\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_img(image)\n",
        "    detections = inference_model.predict(input_image)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [\n",
        "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "    ]\n",
        "    visualize_detections(\n",
        "        image,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
