{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h0JTdx7zssVc"
      },
      "source": [
        "# Object Detection\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dIT9DF_vsz2n"
      },
      "source": [
        "### import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rNcsB8K-8AG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_jHFpTUTtdrf"
      },
      "source": [
        "### Downloading the COCO2017 dataset\n",
        "-\n",
        "Training on the entire COCO2017 dataset which has around 118k images takes a lot of time, hence we will be using a smaller subset of ~500 images for training in this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3n7sxVYssGE"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
        "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
        "keras.utils.get_file(filename, url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt9Ul7g2uC77"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"data.zip\" , \"r\") as f:\n",
        "  f.extractall(\"./\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zuOFzj8Ywn8Y"
      },
      "source": [
        "### Implementing utility functions\n",
        "Bounding boxes can be represented in multiple ways, the most common formats are:\n",
        "\n",
        "- Storing the coordinates of the corners [xmin, ymin, xmax, ymax]\n",
        "- Storing the coordinates of the center and the box dimensions [x, y, width, height]\n",
        "\n",
        "Since we require both formats, we will be implementing functions for converting between the formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWzBPxn_uZX7"
      },
      "outputs": [],
      "source": [
        "def swap_xy(boxes):\n",
        "  return tf.stack([boxes[:,1], boxes[:,0], boxes[:,3], boxes[:,2]], axis = -1)\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "  return tf.concat(\n",
        "      [(boxes[...,2:] + boxes[...,:2]) / 2.0, boxes[...,2:] - boxes[...,:2]], axis = -1\n",
        "  )\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "  return tf.concat(\n",
        "      [boxes[...,2:] - boxes[...,:2] / 2.0, boxes[...,2:] + boxes[...,:2] / 2.0], axis = -1\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eQQI8NKB0kwU"
      },
      "source": [
        "### IOU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZSxbVVszs4V"
      },
      "outputs": [],
      "source": [
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a3lEZjn2VAv"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-jv7sHquhR"
      },
      "source": [
        "###Implementing Anchor generator\n",
        "-\n",
        "Anchor boxes are fixed sized boxes that the model uses to predict the bounding box for an object. It does this by regressing the offset between the location of the object's center and the center of an anchor box, and then uses the width and height of the anchor box to predict a relative scale of the object. In the case of RetinaNet, each location on a given feature map has nine anchor boxes (at three scales and three ratios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMiQlXfm3a1V"
      },
      "outputs": [],
      "source": [
        "class AnchorBox():\n",
        "  def __init__(self):\n",
        "    self.aspect_ratios  = [0.5, 1.0, 2.0]\n",
        "    self.sclaes = [2 ** x for x in [0, 1/3, 2/3]]\n",
        "    self.num_anchor = len(self.aspect_ratios) * len(self.sclaes)\n",
        "    self.strides = [2 ** i for i in range(3, 8)]\n",
        "    self.areas = [ x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "    self._anchor_dims =  self._compute_dims()\n",
        "\n",
        "  def _compute_dims(self):\n",
        "    anchor_dims_all = []\n",
        "    for area in self.areas:\n",
        "      anchor_dims = []\n",
        "      for ratio in self.ratios:\n",
        "        anchor_height = tf.math.sqrt(area/ratio)\n",
        "        anchor_width = area / anchor_height\n",
        "        dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "        for scale in self.scales:\n",
        "          anchor_dims.append(scale*dims)\n",
        "      anchor_dims_all.append(tf.stack(anchor_dims, axis =-2))\n",
        "    return anchor_dims_all\n",
        "\n",
        "  def _get_anchors(self, feature_height, feature_width, level):\n",
        "    rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "    ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "    centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "    centers = tf.expand_dims(centers, axis=-2)\n",
        "    centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "    dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "    anchors = tf.concat([centers, dims], axis=-1)\n",
        "    return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "      )\n",
        "\n",
        "  def get_anchors(self, image_height, image_width):\n",
        "    anchors = [\n",
        "        self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "    for i in range(3, 8)\n",
        "        ]\n",
        "    return tf.concat(anchors, axis=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ywdNC83Oqml9"
      },
      "source": [
        "### Preprocessing data\n",
        "Preprocessing the images involves two steps:\n",
        "\n",
        "- Resizing the image: Images are resized such that the shortest size is equal to 800 px, after resizing if the longest side of the image exceeds 1333 px, the image is resized such that the longest size is now capped at 1333 px.\n",
        "- Applying augmentation: Random scale jittering and random horizontal flipping\n",
        "are the only augmentations applied to the images.\n",
        "\n",
        "Along with the images, bounding boxes are rescaled and flipped if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyVUs15snBWL"
      },
      "outputs": [],
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
        "  return image, boxes\n",
        "\n",
        "def resize_and_pad_image(\n",
        "      image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "  ):\n",
        "  image_shape = tf.cast(tf.shape(image)[:2], dtype = tf.float32)\n",
        "  if jitter is not None:\n",
        "    min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "  ratio = min_side / tf.reduce_min(image_shape)\n",
        "  if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "    ratio = max_side / tf.reduce_max(image_shape)\n",
        "  image_shape = ratio * image_shape\n",
        "  image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "  padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "  image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "  return image, image_shape, ratio\n",
        "\n",
        "def preprocess_data(sample):\n",
        "  image = sample[\"image\"]\n",
        "  bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "  class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "  image, bbox = random_flip_horizontal(image, bbox)\n",
        "  image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "  bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "  bbox = convert_to_xywh(bbox)\n",
        "  return image, bbox, class_id\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### resize_and_pad_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_backbone():\n",
        "    backbone = tf.keras.apllications.RasNet50(include_top=False, image_size=[None, None, 3])\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return(\n",
        "        keras.Model(inputs = [backbone.input], outputs = [c3_output, c4_output, c5_output])\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Feature Pyramid Network as a custom layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFeaturePyramid\u001b[39;00m(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLayer):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, backbone\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFeaturePyramid\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "class FeaturePyramid(keras.layers.Layer):\n",
        "    def __init__(self, backbone=None, **kwargs):\n",
        "        super().__init__(name = \"FeaturePyramid\" **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "    \n",
        "    def call(self, image, training = False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(image, training = training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
